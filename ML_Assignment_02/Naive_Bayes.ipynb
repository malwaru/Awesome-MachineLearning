{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-block alert-info\" >\n",
    "    <h1>Machine Learning: Assignment 2</h1>\n",
    "    <h2>This is a two week assignment</h2>\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Feel free to add cells if required.<br> Feel free to write your own function block to reduce the redundancy.<br> Answers belong into the corresponding cells (below the question). <br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>). We don't consider that respective solution if you make this mistake (no excuse).</p>\n",
    "    <br><br><b> Plot should have axis labels, grid, legend, title, atleast size 10X10 .Also give proper comments, function name, variable names to your coding, if you didn't follow the instructions there will be a reduction in the points.</b><br><br> \n",
    "     <br><br><b> Write sudo-code if you didn't get output or left out of time so that you will be awarded with atmost 50% of marks for that particular session.</b><br><br>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Use the following naming convention for your submissions: LA_FirstnameLastname_dateOfLecture, e.g LA_JohnDoe_YYMMDD\n",
    "     <br><br>Please submit your notebook via LEA. The assignment is due on <b>$1^{st}$ May, Saturday at 18:00.</b> </p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to two people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <p><b>YOU SHOULD ONLY SUBMIT EXACTLY ONE PER GROUP</b></p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the UID (i.e. student2s) of each team member into the variables. \n",
    "If you work alone please leave the second variable empty, or extend the list if necessary.\n",
    "'''\n",
    "member1 = ''\n",
    "member2 = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Build a spam classifier using Naive Bayes[100 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sd/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Headers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import sklearn\n",
    "#Include your other headers here\n",
    "import csv\n",
    "from typing import List, Dict\n",
    "from classifier import NaiveBayesClassifier\n",
    "\n",
    "from nltk import download\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# needs to download Lemmatizer dataset!\n",
    "_ = download('wordnet')\n",
    "_ = download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Step 1:- Load your data[10 points]\n",
    "#### There are three datasets for training: TrainDataset1.csv, TrainDataset2.csv and TrainDataset3.txt. Each dataset contains short messages with the labels (ham or spam). Load the dataset using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5559 entries in 'TrainDataset1.csv'.\n",
      "Found 4457 entries in 'TrainDataset2.csv'.\n",
      "Found 5574 entries in 'TrainDataset3.txt'.\n",
      "Dataset contains 2083 spam entries and 13507 valid entries.\n"
     ]
    }
   ],
   "source": [
    "# NOT USING PANDAS - i dont find it useful in this case! \n",
    "# Renamed function to be consisten with python naming conventions.\n",
    "def load_data() -> List[Dict[str, str]]:\n",
    "    \n",
    "    raw_data = []\n",
    "\n",
    "    with open(\"TrainDataset1.csv\", \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        # do not need the header\n",
    "        next(reader)\n",
    "\n",
    "        for row in reader:\n",
    "            raw_data.append({\"type\": row[0], \"text\": row[1]})\n",
    "        \n",
    "    size = len(raw_data)\n",
    "    print(f\"Found {size} entries in 'TrainDataset1.csv'.\")\n",
    "\n",
    "    \n",
    "    with open(\"TrainDataset2.csv\", \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "                \n",
    "        # do not need the header\n",
    "        next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            raw_data.append({\"type\": row[0], \"text\": row[1]})\n",
    "            \n",
    "            \n",
    "    print(f\"Found {len(raw_data) - size} entries in 'TrainDataset2.csv'.\")\n",
    "    size = len(raw_data)\n",
    "    \n",
    "    \n",
    "    with open(\"TrainDataset3.txt\", \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "                      \n",
    "        for row in reader:\n",
    "            splitted = row[0].split(\"\\t\")\n",
    "            raw_data.append({\"type\": splitted[0], \"text\": splitted[1]})\n",
    "            \n",
    "            \n",
    "    print(f\"Found {len(raw_data) - size} entries in 'TrainDataset3.txt'.\")\n",
    "\n",
    "    \n",
    "    return raw_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "raw_data = load_data()\n",
    "count_spams = len([1 for row in raw_data if row['type'] == \"spam\"])\n",
    "\n",
    "print(f\"Dataset contains {count_spams} spam entries and {len(raw_data)-count_spams} valid entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Step 2:- Preprocess the data[20 points]\n",
    "#### Analyse the data, for this you will need to process the text, namely remove punctuation and stopwords, and then create a list of clean text words (Research how to do this [Hint:- see how the texts are pre-processed in Natural Language Processing]) use any libraries that you feel comfortable. Now Combine them into one big data set for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def pre_process(raw_data: List[Dict[str, str]], min_word_length: int = 2) -> List[Dict]:\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    cleaned_data = []\n",
    "    \n",
    "    for entry in raw_data:\n",
    "        tokenized = tokenizer.tokenize(entry[\"text\"])\n",
    "        lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "        normalized = [word.lower() for word in lemmatized if word.isalpha() and word not in stop_words]\n",
    "        min_length_words = [word for word in normalized if len(word) > min_word_length]\n",
    "        cleaned_data.append({\"type\": entry[\"type\"], \"words\": min_length_words})\n",
    "           \n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "cleaned_data = pre_process(raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created corpus with 1000 entries (with minimun 20 occurences)\n",
      "\n",
      "\n",
      "Top ten most used words\n",
      "call - freq_spam: 0.59, freq_ham: 0.41, count: 1562, spam: 928, hams: 634\n",
      "get - freq_spam: 0.22, freq_ham: 0.78, count: 1054, spam: 237, hams: 817\n",
      "you - freq_spam: 0.33, freq_ham: 0.67, count: 908, spam: 297, hams: 611\n",
      "free - freq_spam: 0.8, freq_ham: 0.2, count: 757, spam: 603, hams: 154\n",
      "day - freq_spam: 0.11, freq_ham: 0.89, count: 697, spam: 78, hams: 619\n",
      "know - freq_spam: 0.09, freq_ham: 0.91, count: 681, spam: 64, hams: 617\n",
      "good - freq_spam: 0.05, freq_ham: 0.95, count: 662, spam: 30, hams: 632\n",
      "come - freq_spam: 0.02, freq_ham: 0.98, count: 661, spam: 13, hams: 648\n",
      "like - freq_spam: 0.05, freq_ham: 0.95, count: 648, spam: 31, hams: 617\n",
      "time - freq_spam: 0.08, freq_ham: 0.92, count: 646, spam: 53, hams: 593\n",
      "\n",
      "\n",
      "Top ten least used words\n",
      "longer - freq_spam: 0.0, freq_ham: 1.0, count: 21, spam: 0, hams: 21\n",
      "fat - freq_spam: 0.0, freq_ham: 1.0, count: 21, spam: 0, hams: 21\n",
      "heavy - freq_spam: 0.0, freq_ham: 1.0, count: 21, spam: 0, hams: 21\n",
      "character - freq_spam: 0.0, freq_ham: 1.0, count: 21, spam: 0, hams: 21\n",
      "deep - freq_spam: 0.0, freq_ham: 1.0, count: 21, spam: 0, hams: 21\n",
      "kept - freq_spam: 0.43, freq_ham: 0.57, count: 21, spam: 9, hams: 12\n",
      "interested - freq_spam: 0.14, freq_ham: 0.86, count: 21, spam: 3, hams: 18\n",
      "once - freq_spam: 0.0, freq_ham: 1.0, count: 21, spam: 0, hams: 21\n",
      "list - freq_spam: 0.0, freq_ham: 1.0, count: 21, spam: 0, hams: 21\n",
      "learn - freq_spam: 0.29, freq_ham: 0.71, count: 21, spam: 6, hams: 15\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "Top ten most used words in spam\n",
      "call - freq_spam: 0.59, freq_ham: 0.41, count: 1562, spam: 928, hams: 634\n",
      "free - freq_spam: 0.8, freq_ham: 0.2, count: 757, spam: 603, hams: 154\n",
      "txt - freq_spam: 0.91, freq_ham: 0.09, count: 457, spam: 416, hams: 41\n",
      "text - freq_spam: 0.66, freq_ham: 0.34, count: 547, spam: 359, hams: 188\n",
      "mobile - freq_spam: 0.9, freq_ham: 0.1, count: 384, spam: 347, hams: 37\n",
      "stop - freq_spam: 0.75, freq_ham: 0.25, count: 408, spam: 305, hams: 103\n",
      "you - freq_spam: 0.33, freq_ham: 0.67, count: 908, spam: 297, hams: 611\n",
      "claim - freq_spam: 1.0, freq_ham: 0.0, count: 291, spam: 291, hams: 0\n",
      "reply - freq_spam: 0.72, freq_ham: 0.28, count: 397, spam: 284, hams: 113\n",
      "www - freq_spam: 0.99, freq_ham: 0.01, count: 252, spam: 249, hams: 3\n",
      "\n",
      "\n",
      "Top ten most likely words in spam\n",
      "claim - freq_spam: 1.0, freq_ham: 0.0, count: 291, spam: 291, hams: 0\n",
      "prize - freq_spam: 1.0, freq_ham: 0.0, count: 232, spam: 232, hams: 0\n",
      "tone - freq_spam: 1.0, freq_ham: 0.0, count: 205, spam: 205, hams: 0\n",
      "guaranteed - freq_spam: 1.0, freq_ham: 0.0, count: 137, spam: 137, hams: 0\n",
      "awarded - freq_spam: 1.0, freq_ham: 0.0, count: 103, spam: 103, hams: 0\n",
      "ringtone - freq_spam: 1.0, freq_ham: 0.0, count: 91, spam: 91, hams: 0\n",
      "entry - freq_spam: 1.0, freq_ham: 0.0, count: 71, spam: 71, hams: 0\n",
      "mob - freq_spam: 1.0, freq_ham: 0.0, count: 71, spam: 71, hams: 0\n",
      "weekly - freq_spam: 1.0, freq_ham: 0.0, count: 68, spam: 68, hams: 0\n",
      "won - freq_spam: 1.0, freq_ham: 0.0, count: 66, spam: 66, hams: 0\n",
      "\n",
      "\n",
      "Top ten most used words in ham\n",
      "get - freq_spam: 0.22, freq_ham: 0.78, count: 1054, spam: 237, hams: 817\n",
      "come - freq_spam: 0.02, freq_ham: 0.98, count: 661, spam: 13, hams: 648\n",
      "call - freq_spam: 0.59, freq_ham: 0.41, count: 1562, spam: 928, hams: 634\n",
      "good - freq_spam: 0.05, freq_ham: 0.95, count: 662, spam: 30, hams: 632\n",
      "day - freq_spam: 0.11, freq_ham: 0.89, count: 697, spam: 78, hams: 619\n",
      "know - freq_spam: 0.09, freq_ham: 0.91, count: 681, spam: 64, hams: 617\n",
      "like - freq_spam: 0.05, freq_ham: 0.95, count: 648, spam: 31, hams: 617\n",
      "you - freq_spam: 0.33, freq_ham: 0.67, count: 908, spam: 297, hams: 611\n",
      "got - freq_spam: 0.03, freq_ham: 0.97, count: 624, spam: 19, hams: 605\n",
      "time - freq_spam: 0.08, freq_ham: 0.92, count: 646, spam: 53, hams: 593\n",
      "\n",
      "\n",
      "Top ten most likely words in ham\n",
      "lor - freq_spam: 0.0, freq_ham: 1.0, count: 450, spam: 0, hams: 450\n",
      "but - freq_spam: 0.0, freq_ham: 1.0, count: 372, spam: 0, hams: 372\n",
      "later - freq_spam: 0.0, freq_ham: 1.0, count: 323, spam: 0, hams: 323\n",
      "said - freq_spam: 0.0, freq_ham: 1.0, count: 237, spam: 0, hams: 237\n",
      "ask - freq_spam: 0.0, freq_ham: 1.0, count: 236, spam: 0, hams: 236\n",
      "morning - freq_spam: 0.0, freq_ham: 1.0, count: 210, spam: 0, hams: 210\n",
      "lol - freq_spam: 0.0, freq_ham: 1.0, count: 205, spam: 0, hams: 205\n",
      "anything - freq_spam: 0.0, freq_ham: 1.0, count: 194, spam: 0, hams: 194\n",
      "feel - freq_spam: 0.0, freq_ham: 1.0, count: 189, spam: 0, hams: 189\n",
      "something - freq_spam: 0.0, freq_ham: 1.0, count: 181, spam: 0, hams: 181\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class Result():\n",
    "    def __init__(self, *, word: str, count: int = 0, count_spam: int = 0, count_ham: int = 0):\n",
    "        self.word = word\n",
    "        self.count = count\n",
    "        self.count_spam = count_spam\n",
    "        self.count_ham = count_ham\n",
    "        self.freq_spam = 0.0\n",
    "        self.freq_ham = 0.0\n",
    "        \n",
    "           \n",
    "    @classmethod\n",
    "    def from_spam(cls, word):\n",
    "        return Result(word=word, count=1, count_spam=1)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_ham(cls, word):\n",
    "        return Result(word=word, count=1, count_ham=1)\n",
    "    \n",
    "    def add_spam(self):\n",
    "        self.count += 1\n",
    "        self.count_spam += 1   \n",
    "    \n",
    "    def add_ham(self):\n",
    "        self.count += 1\n",
    "        self.count_ham += 1\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.word} - freq_spam: {round(self.freq_spam, 2)}, freq_ham: {round(self.freq_ham, 2)}, count: {self.count}, spam: {self.count_spam}, hams: {self.count_ham}\"\n",
    "    \n",
    "    def calculate_frequencies(self):\n",
    "        self.freq_spam = float(self.count_spam) / float(self.count)\n",
    "        self.freq_ham = float(self.count_ham) / float(self.count)\n",
    "        \n",
    "    \n",
    "        \n",
    "def build_corpus(cleaned_data, min_occurences: int = 20):\n",
    "    counts = {}\n",
    "    results = {}\n",
    "    \n",
    "    \n",
    "    for row in cleaned_data:\n",
    "        for word in row['words']:\n",
    "            if word not in counts:\n",
    "                if row[\"type\"] == \"spam\":\n",
    "                    counts[word] = Result.from_spam(word)\n",
    "                else:\n",
    "                    counts[word] = Result.from_ham(word)\n",
    "            else:\n",
    "                if row[\"type\"] == \"spam\":\n",
    "                    counts[word].add_spam()\n",
    "                else:\n",
    "                    counts[word].add_ham()\n",
    "\n",
    "                \n",
    "    \n",
    "    for word, result in counts.items():\n",
    "        if result.count > min_occurences:\n",
    "            result.calculate_frequencies()\n",
    "            results[word] = result\n",
    "            \n",
    "    return results\n",
    "\n",
    "corpus = build_corpus(cleaned_data)\n",
    "    \n",
    "print(f\"Created corpus with {len(corpus.keys())} entries (with minimun 20 occurences)\")\n",
    "\n",
    "list_corpus = [value for key, value in corpus.items()]\n",
    "\n",
    "print(\"\\n\\nTop ten most used words\")\n",
    "list_corpus.sort(key=lambda x: x.count, reverse=True)\n",
    "for w in list_corpus[:10]:\n",
    "    print(w)\n",
    "    \n",
    "\n",
    "print(\"\\n\\nTop ten least used words\")\n",
    "list_corpus.sort(key=lambda x: x.count)\n",
    "for w in list_corpus[:10]:\n",
    "    print(w)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"-\"*20)\n",
    "    \n",
    "    \n",
    "print(\"\\n\\nTop ten most used words in spam\")\n",
    "list_corpus.sort(key=lambda x: x.count_spam, reverse=True)\n",
    "for w in list_corpus[:10]:\n",
    "    print(w)\n",
    "    \n",
    "print(\"\\n\\nTop ten most likely words in spam\")\n",
    "list_corpus.sort(key=lambda x: x.freq_spam, reverse=True)\n",
    "for w in list_corpus[:10]:\n",
    "    print(w)\n",
    "    \n",
    "print(\"\\n\\nTop ten most used words in ham\")\n",
    "list_corpus.sort(key=lambda x: x.count_ham, reverse=True)\n",
    "for w in list_corpus[:10]:\n",
    "    print(w)\n",
    "    \n",
    "print(\"\\n\\nTop ten most likely words in ham\")\n",
    "list_corpus.sort(key=lambda x: x.freq_ham, reverse=True)\n",
    "for w in list_corpus[:10]:\n",
    "    print(w)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Step 3:- Visualise the data[20 points]\n",
    "#### Try to visualize and analyse the data such as before and after pre processing, number of ham/spam etc. Analyse as many verticals you can, feel free to use graphical libraries like seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "#Visualise the data   \n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Step 4:- Build, train and validate the classifer, [20 points]\n",
    "\n",
    "### Training on supervised data (labelled data)\n",
    "\n",
    "#### Use the data in order to build your own Naive Bayes classifier (You can either use existing Naive Bayes from sklearn or build your own). Build the classifier, train it and then validate. Provide your result in confusion matrix (use heatmap from seaborn) along with the classification report from sklearn. Validation accuracy should be around 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Build, train and validate the classifier, \n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Step 5:- Test the classifier[10 points]\n",
    "\n",
    "### Supervised classification[5 points] \n",
    "\n",
    "#### Test your Classifier using  the SMSSpamCollection.txt dataset provide a heatmap and classification report. Test accuracy should be around 99%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Test the classifier\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Unsupervised classification[5 points] \n",
    "\n",
    "#### Test your Classifier using  the TestDataset.csv dataset. This dataset is not labelled so kindly predict the labels and visualise it[5 points]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Test the classifier\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Step 6:- Cheat the classifier[20 points]\n",
    "\n",
    "#### Try to cheat the classifier by adding \"good words\" to the end of test dataset(TestDataset.csv) e.g:- Oh! no share Market has fallen down by $100,000 due to Corona outbreak... try mixing up spam and ham words see how the classifier works. Output the results in a good format to validate your work[15 points]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Cheat the classifier\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Write your analysis of how you intended to cheat the classifier and how it performed in few words (provide your inference)[5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**Give your expalanation here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Help\n",
    "\n",
    "<a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering\" target=\"_top\">Spam Filtering using Naive Bayes</a><br>\n",
    "<a href=\"https://seaborn.pydata.org/generated/seaborn.heatmap.html\" target=\"_top\">Seaborn Heatmap</a><br>\n",
    "<a href=\"https://scikit-learn.org/stable/modules/naive_bayes.html\" target=\"_top\">Sklearn Naive Bayes</a><br>\n",
    "<a href=\"https://scikit-learn.org/stable/modules/model_evaluation.html\" target=\"_top\">Sklearn Metrics</a><br>\n",
    "<a href=\"https://pandas.pydata.org/docs/getting_started/index.html#getting-started\" target=\"_top\">Intro to Pandas</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
