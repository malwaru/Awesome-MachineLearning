{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2980977",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a812b390a6bc7f26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Hochschule Bonn-Rhein-Sieg\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "# Reinforcement Learning Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c3c07",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-db895a3175409b48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before you start working on this assignment, please:\n",
    "* set up the [OpenAI gym library](https://gym.openai.com), which is a Python library that defines various benchmarking environments in which reinforcement learning agents can be trained. Installation instructions for the library can be found [here](https://gym.openai.com/docs/#installation)\n",
    "* go through the [\"Getting Started With Gym\" tutorial](https://gym.openai.com/docs/#getting-started-with-gym) so that you can use the library effectively during the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b4ab8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e62e4c2eeced11c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7ec73",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-87160b9f59e6aaa4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q-Learning\n",
    "\n",
    "Your task in this assignment is, in principle, rather simple: You need to implement the Q-learning algorithm for solving the cart-pole problem that we discussed in the reinforcement learning lecture.\n",
    "\n",
    "As you might remember from the lecture, the observation space of the cart-pole system consists of:\n",
    "1. the cart's $x$ position on the track\n",
    "2. the cart's linear velocity $\\dot{x}$ along the track\n",
    "3. the pole's angle $\\theta$ with respect to the vertical axis\n",
    "4. the pole's angular velocity $\\dot{\\theta}$\n",
    "\n",
    "The action space of the system is discrete - the pole can move either to the left or to the right with a constant velocity.\n",
    "\n",
    "The cart-pole environment provided in OpenAI gym is episodic, such that an episode ends if the pole (i) falls beyond a certain angle (the agent has failed in this case) or (ii) doesn't fall for 200 consecutive steps (the agent has succeeded in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16296aea",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c7c15fbe487e42f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q-Function Learning [80 points]\n",
    "\n",
    "Your main task is to implement the function `train_policy` below, which takes the cart-pole environment provided in OpenAI gym and learns the values of the Q-function $Q(s,a)$ so that the cart-pole problem can be solved.\n",
    "\n",
    "#### State discretisation\n",
    "\n",
    "We particularly want to implement a discrete version of Q-learning, namely the Q-function $Q(s,a)$ should be learned for a discretised version of the state space. This means that you need to choose a discretisation of the state space variables so that you can then learn a tabular version of $Q(s,a)$ for all possible combinations of $s$ and $a$.\n",
    "\n",
    "You don't need to use all four state variables in your Q-function if some don't seem to contribute to the performance of the agent. This is something you will need to experiment with in your implementation.\n",
    "\n",
    "#### Action selection during training\n",
    "\n",
    "How you select actions during training is another aspect that you need to choose and experiment with. Remember that Q-learning is an off-policy learning algorithm, so it doesn't always choose the best action according to the current Q-values. A commonly used strategy is to use an $\\epsilon$-greedy policy during learning, according to which a greedy action (the one with the highest Q-value) is selected most of the time, but a random action is chosen with a probability $p$, where $p$ starts large at the beginning and decreases according to some schedule during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d24c51",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q_learning_implementation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_policy(env, learning_episodes: int) -> np.ndarray:\n",
    "    \"\"\"Trains a policy for the given environment using tabular Q-learning.\n",
    "\n",
    "    Keyword arguments:\n",
    "    env -- OpenAI gym environment\n",
    "    learning_episodes: int -- how many episodes to train the agent for\n",
    "\n",
    "    Returns:\n",
    "    Q -- a multidimensional numpy array representing the Q function Q(s,a)\n",
    "\n",
    "    \"\"\"\n",
    "    Q = np.zeros(0)\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9e060",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0bca8c9c6f2eb2d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Please run the following cell to execute your Q-function implementation. The learned Q-values will be used below for testing the agent. Note that learning may take several minutes. You are allowed to change the number of learning episodes to fit your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba59ba3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7c4507c48ab2c241",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "learning_episodes = 300000\n",
    "\n",
    "print('Training agent for {0} episodes...'.format(learning_episodes))\n",
    "Q = train_policy(env, learning_episodes)\n",
    "print('Training completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ce957",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb3ca376db471f37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Agent Testing [20 points]\n",
    "\n",
    "Now that you have learned a Q-function for the cart-pole problem, you need to test your implementation. For this, please implement the function `test_policy`, which runs the agent for a given number of test episodes using the learned Q-values and returns the average return over the test episodes.\n",
    "\n",
    "You can consider the cart-pole problem solved if your average return is $\\geq 195$, which means that your agent is able to survive until the end of each episode most of the time.\n",
    "\n",
    "If you notice that the average return is lower than expected, there might be different issues with the implementation, for instance:\n",
    "* the number of training episodes might be low\n",
    "* the action selection during training may be inappropriate (try experimenting with different schedules for your epsilon-greedy policy)\n",
    "* your state space discretisation may be inappropriate (also, the more fine-grained the discretisation is, the more training episodes you will need)\n",
    "\n",
    "You may need to experiment with all of these until you successfully train your agent.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620e022",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2f79255a005b51a6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_policy(env, Q: np.ndarray, test_episodes: int) -> float:\n",
    "    \"\"\"Test a policy extracted from a learned Q-function on the given environment.\n",
    "    The policy is tested for a given number of episodes.\n",
    "\n",
    "    Keyword arguments:\n",
    "    env -- OpenAI gym environment\n",
    "    Q: np.ndarray -- a multidimensional numpy array representing the Q function Q(s,a)\n",
    "    test_episodes: int -- number of episodes for testing the learned policy\n",
    "\n",
    "    Returns:\n",
    "    avg_return: float -- the average return over the test episodes\n",
    "\n",
    "    \"\"\"\n",
    "    avg_return = 0.\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return avg_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276d13a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-409f3bf92805ad7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_episodes = 100\n",
    "\n",
    "print('Testing policy for {0} episodes...'.format(test_episodes))\n",
    "avg_return = test_policy(env, Q, test_episodes)\n",
    "print('Average test return =', avg_return)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
